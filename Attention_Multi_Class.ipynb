{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_ssmze_Pj8W7",
    "outputId": "d719af0d-0742-4852-fdfa-2b7137c8c70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpTsY7NXlUXE"
   },
   "source": [
    "# **Importing Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlCiqujokpZt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input,LSTM, Bidirectional, Dropout,Conv1D, GlobalMaxPool1D\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.optimizers import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jnqfW_4lZpB"
   },
   "source": [
    "# **Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxL0a8pcjl7i"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwWQRiIzlp_s"
   },
   "source": [
    "# **Data Loading / Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "LfW0CQ4Ilse8",
    "outputId": "b4c1e14e-0673-4cbe-82b4-a9554740ddbc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>para</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18 it is further noted by us that identical is...</td>\n",
       "      <td>royalty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8 41for taxation purposes the term royalty has...</td>\n",
       "      <td>royalty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>51it is found that all the issues raised by th...</td>\n",
       "      <td>royalty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the phrase computer software is commonly used ...</td>\n",
       "      <td>royalty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9 we heard rival submissions and perused the m...</td>\n",
       "      <td>royalty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               para    label\n",
       "0           0  18 it is further noted by us that identical is...  royalty\n",
       "1           1  8 41for taxation purposes the term royalty has...  royalty\n",
       "2           2  51it is found that all the issues raised by th...  royalty\n",
       "3           3  the phrase computer software is commonly used ...  royalty\n",
       "4           4  9 we heard rival submissions and perused the m...  royalty"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"drive/My Drive/MI22_cleaned2.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb1eqzHwlz6B"
   },
   "outputs": [],
   "source": [
    "data.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sNL5-sCl2mQ"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data,test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQCcTcyKl58R"
   },
   "outputs": [],
   "source": [
    "# Preparaing Targets\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train[\"label\"].values)\n",
    "y_train = encoder.transform(train[\"label\"].values)\n",
    "y_test = encoder.transform(test[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ElVLIZPQl9cl",
    "outputId": "a3532de6-0b78-475d-abf2-7d17815429fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Text Featurization\n",
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(\"drive/My Drive/ content glove glove.42B.300d.txt\") as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpRKS6SBmTPg"
   },
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_sHxDmzmXYc"
   },
   "outputs": [],
   "source": [
    "# convert the sentences (strings) into integers\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train[\"para\"])\n",
    "sequences_train = tokenizer.texts_to_sequences(train[\"para\"])\n",
    "sequences_test = tokenizer.texts_to_sequences(test[\"para\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b7jL_dFumZur",
    "outputId": "dc42430a-28e5-4500-abe9-407c71309e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 324477 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRVxvfKKmcZM"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2P_8UaykmgV1",
    "outputId": "d0151ab4-1a72-4877-e731-bf12d3a1b38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (34311, 600)\n",
      "Shape of data tensor: (8578, 600)\n"
     ]
    }
   ],
   "source": [
    "encoded_train = pad_sequences(sequences_train,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', encoded_train.shape)\n",
    "encoded_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', encoded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mUyRvJt5mkGS",
    "outputId": "c6d82023-122e-44fb-80aa-dcd8e480adf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n",
      "(20000, 300)\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "  if i < MAX_VOCAB_SIZE:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l96jGIRlnmdr"
   },
   "source": [
    "# **Model Building // Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yIcJwxBnpD8"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=600,\n",
    "  trainable=False\n",
    ")\n",
    "input_ = Input(shape=(600,))\n",
    "x = embedding_layer(input_)\n",
    "e = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "e = Conv1D(50, (1), activation='relu')(e)\n",
    "e = Conv1D(50, (1), activation='relu')(e)\n",
    "e = Bidirectional(LSTM(50, return_sequences=True))(e)\n",
    "e = Conv1D(50, (1), activation='relu')(e)\n",
    "e = Conv1D(50, (1), activation='relu')(e)\n",
    "e = Attention(600)(e)\n",
    "output = Dense(19, activation=\"softmax\")(e)\n",
    "model = Model(input_,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "8zcPMWxzotov",
    "outputId": "52ca00da-7189-400a-811c-46c0268b58fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 600, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 600, 100)          140400    \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 600, 50)           5050      \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 600, 50)           2550      \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 600, 100)          40400     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 600, 50)           5050      \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 600, 50)           2550      \n",
      "_________________________________________________________________\n",
      "attention_7 (Attention)      (None, 50)                650       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 19)                969       \n",
      "=================================================================\n",
      "Total params: 6,197,619\n",
      "Trainable params: 197,619\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY-t4a6_oxOz"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_2 = ModelCheckpoint('model_2.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "logdir = \"logs/model_/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjefHGyUo9RU"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "uf3kmHWVo_lM",
    "outputId": "1cc03095-b202-43a8-97c0-b7b93f63e7d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train on 34311 samples, validate on 8578 samples\n",
      "Epoch 1/10\n",
      "34311/34311 [==============================] - 337s 10ms/step - loss: 1.9748 - acc: 0.5288 - val_loss: 1.8695 - val_acc: 0.5372\n",
      "Epoch 2/10\n",
      "34311/34311 [==============================] - 328s 10ms/step - loss: 1.5820 - acc: 0.5576 - val_loss: 1.3599 - val_acc: 0.5983\n",
      "Epoch 3/10\n",
      "34311/34311 [==============================] - 332s 10ms/step - loss: 1.2168 - acc: 0.6195 - val_loss: 1.1886 - val_acc: 0.6303\n",
      "Epoch 4/10\n",
      "34311/34311 [==============================] - 334s 10ms/step - loss: 0.9640 - acc: 0.6988 - val_loss: 1.0567 - val_acc: 0.6878\n",
      "Epoch 5/10\n",
      "34311/34311 [==============================] - 334s 10ms/step - loss: 0.7990 - acc: 0.7509 - val_loss: 0.7784 - val_acc: 0.7621\n",
      "Epoch 6/10\n",
      "34311/34311 [==============================] - 331s 10ms/step - loss: 0.7322 - acc: 0.7858 - val_loss: 0.8514 - val_acc: 0.7474\n",
      "Epoch 7/10\n",
      "34311/34311 [==============================] - 334s 10ms/step - loss: 0.6098 - acc: 0.8113 - val_loss: 0.7392 - val_acc: 0.7858\n",
      "Epoch 8/10\n",
      "34311/34311 [==============================] - 330s 10ms/step - loss: 0.5357 - acc: 0.8357 - val_loss: 0.7978 - val_acc: 0.7799\n",
      "Epoch 9/10\n",
      "34311/34311 [==============================] - 326s 9ms/step - loss: 0.5525 - acc: 0.8327 - val_loss: 0.7226 - val_acc: 0.7863\n",
      "Epoch 10/10\n",
      "34311/34311 [==============================] - 326s 9ms/step - loss: 0.5111 - acc: 0.8424 - val_loss: 0.6725 - val_acc: 0.8055\n"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  encoded_train,\n",
    "  y_train,\n",
    "  batch_size=512,\n",
    "  epochs=10,\n",
    "  validation_data=(encoded_test,y_test),callbacks=[tensorboard_callback,model_2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJP7BD-gpCCa"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYcLDF7U-9cV"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLbg42Ui-UWg"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "zmfrJoTH-ZCf",
    "outputId": "3064842d-478f-4546-c62f-55f45134c439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.60      0.56       182\n",
      "           1       0.50      0.71      0.59       399\n",
      "           2       0.52      0.44      0.48       204\n",
      "           3       1.00      0.03      0.07        59\n",
      "           4       0.33      0.37      0.34       142\n",
      "           5       0.94      0.89      0.92       412\n",
      "           6       0.94      0.49      0.65       206\n",
      "           7       0.79      0.44      0.57       204\n",
      "           8       0.92      0.89      0.90        88\n",
      "           9       0.78      0.85      0.81       185\n",
      "          10       0.65      0.76      0.70       201\n",
      "          11       0.66      0.41      0.50       185\n",
      "          12       0.75      0.27      0.40       293\n",
      "          13       0.42      0.37      0.39       190\n",
      "          14       0.66      0.56      0.61       300\n",
      "          15       0.89      0.98      0.93      4601\n",
      "          16       0.77      0.56      0.65       285\n",
      "          17       0.76      0.93      0.84       268\n",
      "          18       0.95      0.74      0.83       174\n",
      "\n",
      "    accuracy                           0.81      8578\n",
      "   macro avg       0.72      0.59      0.62      8578\n",
      "weighted avg       0.81      0.81      0.79      8578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(np.argmax(y_test,axis=1),np.argmax(predictions,axis=1)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Attention Multi Class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
