{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SS4K5Yn7Boch"
   },
   "source": [
    "![HAN](https://github.com/ShawnyXiao/TextClassification-Keras/raw/master/image/HAN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "xMo5f-4jAiwQ",
    "outputId": "b34c60b6-a761-419a-ed66-e707f4eaee67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "TVaoxPNP_xLV",
    "outputId": "e0fa4f1f-af75-47ec-8435-fa0d94a27b7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.layers import Input, Dense, TimeDistributed, GRU, Embedding, Dropout, Bidirectional, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcU3FNjJB6gu"
   },
   "outputs": [],
   "source": [
    "# Attention \n",
    "\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYEmR-EcB8Iu"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"drive/My Drive/MI22_cleaned2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-lIhnWcC6P-"
   },
   "outputs": [],
   "source": [
    "data.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpIOzhYiDCcR"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data,test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-n9LcGQDFii"
   },
   "outputs": [],
   "source": [
    "# Preparaing Targets\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train[\"label\"].values)\n",
    "y_train = encoder.transform(train[\"label\"].values)\n",
    "y_test = encoder.transform(test[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WxAS1nSWDIVr"
   },
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen_sentence = 250\n",
    "maxlen_word = 10\n",
    "batch_size = 512\n",
    "embedding_dims = 300\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZ9ETxWLDe9F"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train[\"para\"])\n",
    "sequences_train = tokenizer.texts_to_sequences(train[\"para\"])\n",
    "sequences_test = tokenizer.texts_to_sequences(test[\"para\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "oHP53pZODaUq",
    "outputId": "41792891-94fe-4e9f-cf9a-b3406f499f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x #sentence x #word)...\n",
      "x_train shape: (34311, 250, 10)\n",
      "x_test shape: (8578, 250, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x #sentence x #word)...')\n",
    "x_train = pad_sequences(sequences_train, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_test = pad_sequences(sequences_test, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
    "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Nf70MfNnVY8w",
    "outputId": "1b0ce1e3-c69a-41a6-a49d-13eb0962dbdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(\"drive/My Drive/ content glove glove.42B.300d.txt\") as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5Dx4CqftY2Rc",
    "outputId": "71db5bee-cf09-46e1-f180-c7dcb47f61d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 324477 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dgm0VEf5YiUG",
    "outputId": "34873e78-98a1-45f9-b9f1-fcd07a51d98d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n",
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(max_features, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dims))\n",
    "for word, i in word2idx.items():\n",
    "  if i < max_features:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXnB21CpDxGB"
   },
   "outputs": [],
   "source": [
    "input_word = Input(shape=(maxlen_word,))\n",
    "x_word = Embedding(max_features, embedding_dims, input_length=maxlen_word,weights=[embedding_matrix],trainable=False)(input_word)\n",
    "word = Bidirectional(LSTM(128, return_sequences=True))(x_word)  # LSTM or GRU\n",
    "x_word = Attention(maxlen_word)(x_word)\n",
    "model_word = Model(input_word, x_word)\n",
    "\n",
    "# Sentence part\n",
    "input = Input(shape=(maxlen_sentence, maxlen_word))\n",
    "x_sentence = TimeDistributed(model_word)(input)\n",
    "x_sentence = Bidirectional(LSTM(128, return_sequences=True))(x_sentence)  # LSTM or GRU\n",
    "x_sentence = Attention(maxlen_sentence)(x_sentence)\n",
    "\n",
    "output = Dense(19, activation=\"softmax\")(x_sentence)\n",
    "model = Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCWsv4thLnoB"
   },
   "outputs": [],
   "source": [
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eb82wJDYMBfe"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_1 = ModelCheckpoint('model_han.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "logdir = \"logs/model_han/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "n8R_mA0mnbt4",
    "outputId": "f3a3d9b6-0b86-49a4-826d-73568d33ad90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 250, 10)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 250, 300)          1500310   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 250, 256)          439296    \n",
      "_________________________________________________________________\n",
      "attention_6 (Attention)      (None, 256)               506       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                4883      \n",
      "=================================================================\n",
      "Total params: 1,944,995\n",
      "Trainable params: 444,995\n",
      "Non-trainable params: 1,500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "5Wof7WY7MIRo",
    "outputId": "04af32a0-1025-4bfd-9fec-703b274da9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34311 samples, validate on 8578 samples\n",
      "Epoch 1/20\n",
      "34311/34311 [==============================] - 125s 4ms/step - loss: 0.7517 - acc: 0.7722 - val_loss: 0.7895 - val_acc: 0.7651\n",
      "Epoch 2/20\n",
      "34311/34311 [==============================] - 117s 3ms/step - loss: 0.7574 - acc: 0.7718 - val_loss: 0.8192 - val_acc: 0.7567\n",
      "Epoch 3/20\n",
      "34311/34311 [==============================] - 120s 4ms/step - loss: 0.7626 - acc: 0.7742 - val_loss: 0.7452 - val_acc: 0.7785\n",
      "Epoch 4/20\n",
      "34311/34311 [==============================] - 115s 3ms/step - loss: 0.7005 - acc: 0.7874 - val_loss: 0.7243 - val_acc: 0.7818\n",
      "Epoch 5/20\n",
      "34311/34311 [==============================] - 115s 3ms/step - loss: 0.6639 - acc: 0.7966 - val_loss: 0.7034 - val_acc: 0.7863\n",
      "Epoch 6/20\n",
      "34311/34311 [==============================] - 121s 4ms/step - loss: 0.6415 - acc: 0.8034 - val_loss: 0.6952 - val_acc: 0.7856\n",
      "Epoch 7/20\n",
      "34311/34311 [==============================] - 123s 4ms/step - loss: 0.6211 - acc: 0.8102 - val_loss: 0.6743 - val_acc: 0.7920\n",
      "Epoch 8/20\n",
      "34311/34311 [==============================] - 119s 3ms/step - loss: 0.5924 - acc: 0.8140 - val_loss: 0.6757 - val_acc: 0.7934\n",
      "Epoch 9/20\n",
      "34311/34311 [==============================] - 117s 3ms/step - loss: 0.5746 - acc: 0.8205 - val_loss: 0.6575 - val_acc: 0.7970\n",
      "Epoch 10/20\n",
      "34311/34311 [==============================] - 114s 3ms/step - loss: 0.5699 - acc: 0.8206 - val_loss: 0.6535 - val_acc: 0.8036\n",
      "Epoch 11/20\n",
      "34311/34311 [==============================] - 117s 3ms/step - loss: 0.5939 - acc: 0.8142 - val_loss: 0.6807 - val_acc: 0.7996\n",
      "Epoch 12/20\n",
      "34311/34311 [==============================] - 123s 4ms/step - loss: 0.5701 - acc: 0.8233 - val_loss: 0.6660 - val_acc: 0.8014\n",
      "Epoch 13/20\n",
      "34311/34311 [==============================] - 128s 4ms/step - loss: 0.5391 - acc: 0.8316 - val_loss: 0.6489 - val_acc: 0.8096\n",
      "Epoch 14/20\n",
      "34311/34311 [==============================] - 115s 3ms/step - loss: 0.5204 - acc: 0.8354 - val_loss: 0.6677 - val_acc: 0.7967\n",
      "Epoch 15/20\n",
      "34311/34311 [==============================] - 114s 3ms/step - loss: 0.5810 - acc: 0.8205 - val_loss: 0.6279 - val_acc: 0.8072\n",
      "Epoch 16/20\n",
      "34311/34311 [==============================] - 116s 3ms/step - loss: 0.5153 - acc: 0.8371 - val_loss: 0.6517 - val_acc: 0.8003\n",
      "Epoch 17/20\n",
      "34311/34311 [==============================] - 125s 4ms/step - loss: 0.5331 - acc: 0.8339 - val_loss: 0.6030 - val_acc: 0.8165\n",
      "Epoch 18/20\n",
      "34311/34311 [==============================] - 123s 4ms/step - loss: 0.4770 - acc: 0.8505 - val_loss: 0.5937 - val_acc: 0.8204\n",
      "Epoch 19/20\n",
      "34311/34311 [==============================] - 115s 3ms/step - loss: 0.4594 - acc: 0.8534 - val_loss: 0.6090 - val_acc: 0.8155\n",
      "Epoch 20/20\n",
      "34311/34311 [==============================] - 114s 3ms/step - loss: 0.4893 - acc: 0.8457 - val_loss: 0.5783 - val_acc: 0.8255\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          callbacks=[model_1,tensorboard_callback],\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEC_vYvuMRhg"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "Bpcf8d5hUxwG",
    "outputId": "c9b5e87c-a0a3-4970-9581-adf19020e9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.45      0.54       182\n",
      "           1       0.56      0.65      0.60       399\n",
      "           2       0.65      0.50      0.56       204\n",
      "           3       0.78      0.47      0.59        59\n",
      "           4       0.37      0.34      0.35       142\n",
      "           5       0.91      0.86      0.88       412\n",
      "           6       0.90      0.71      0.79       206\n",
      "           7       0.55      0.50      0.52       204\n",
      "           8       0.90      0.91      0.90        88\n",
      "           9       0.84      0.89      0.87       185\n",
      "          10       0.67      0.74      0.70       201\n",
      "          11       0.58      0.51      0.55       185\n",
      "          12       0.53      0.57      0.55       293\n",
      "          13       0.60      0.48      0.54       190\n",
      "          14       0.57      0.58      0.58       300\n",
      "          15       0.93      0.97      0.95      4601\n",
      "          16       0.76      0.65      0.70       285\n",
      "          17       0.82      0.88      0.85       268\n",
      "          18       0.84      0.80      0.82       174\n",
      "\n",
      "    accuracy                           0.83      8578\n",
      "   macro avg       0.71      0.66      0.68      8578\n",
      "weighted avg       0.82      0.83      0.82      8578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(np.argmax(y_test,axis=1),np.argmax(predictions,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6w01Xx4QVAi_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HAN for Multi Class Classification Attempt 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
